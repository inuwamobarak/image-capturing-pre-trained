{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1acANCUeXTqxYbY+DEw9W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inuwamobarak/image-capturing-pre-trained/blob/main/Image_Caption_Generation_Using_Generative_Artificial_Intelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing requirements"
      ],
      "metadata": {
        "id": "3hfByemHz2b7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9csEh9NyE6h"
      },
      "outputs": [],
      "source": [
        "# Install required packages for the code to run\n",
        "!pip install transformers rouge_score evaluate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and Packages"
      ],
      "metadata": {
        "id": "7ibWfEs6zwkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries and packages\n",
        "\n",
        "import requests  # Library for making HTTP requests\n",
        "import torch  # PyTorch for deep learning\n",
        "from PIL import Image  # Library for image processing\n",
        "from transformers import *  # Transformers library for NLP tasks\n",
        "from tqdm import tqdm  # Library for displaying progress bars\n",
        "import numpy as np # Library for numerical manipulation\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Checking for GPU availability and setting the device accordingly"
      ],
      "metadata": {
        "id": "fW44YhdmybhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Encoders and Decoders"
      ],
      "metadata": {
        "id": "uk6mZald0ZrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model used for encoding the image and extracting image features\n",
        "# Available encoder models:\n",
        "# encoder_model = \"WinKawaks/vit-small-patch16-224\"\n",
        "# encoder_model = \"google/vit-base-patch16-224\"\n",
        "# encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
        "encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
        "\n",
        "# The model used for decoding the image features and generating captions\n",
        "# Available decoder models:\n",
        "# decoder_model = \"bert-base-uncased\"\n",
        "# decoder_model = \"prajjwal1/bert-tiny\"\n",
        "decoder_model = \"gpt2\"\n",
        "\n",
        "## Load the pre-trained Encoder and Decoder models\n",
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    encoder_model, decoder_model\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "orIV7YiIykMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Tokenizers and Image Processors"
      ],
      "metadata": {
        "id": "UMUW2pv61sE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the Tokenizer\n",
        "\n",
        "# The tokenizer is used to preprocess the text and convert it into numerical inputs for the model\n",
        "# Available tokenizers:\n",
        "# tokenizer = AutoTokenizer.from_pretrained(decoder_model)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(decoder_model)\n",
        "\n",
        "## Initialize the Image Processor\n",
        "\n",
        "# The image processor is used to preprocess the images and extract visual features\n",
        "# Available image processors:\n",
        "# - ViTImageProcessor (for \"google/vit-base-patch16-224\" and \"microsoft/swin-base-patch4-window7-224-in22k\" encoder models)\n",
        "image_processor = ViTImageProcessor.from_pretrained(encoder_model)"
      ],
      "metadata": {
        "id": "rNhNiKcx0w7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the Model and Tokenizer for the Decoder"
      ],
      "metadata": {
        "id": "klWjnKQu166B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the decoder model is \"gpt2\"\n",
        "if \"gpt2\" in decoder_model:\n",
        "    # Adjust the tokenizer and model configurations for \"gpt2\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad_token_id as eos_token_id\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model.config.decoder_start_token_id = tokenizer.bos_token_id  # Set decoder_start_token_id as bos_token_id\n",
        "else:\n",
        "    # For other decoder models\n",
        "    model.config.decoder_start_token_id = tokenizer.cls_token_id  # Set decoder_start_token_id as cls_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id  # Set pad_token_id as pad_token_id"
      ],
      "metadata": {
        "id": "hPnmCWIy1x_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and Loading the Dataset"
      ],
      "metadata": {
        "id": "PCPLUZx-2Zyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "max_length = 32  # Maximum length of the captions in tokens\n",
        "coco_dataset_ratio = 50  # 50% of the COCO2014 dataset\n",
        "\n",
        "# Load the COCO2014 dataset for training, validation, and testing splits\n",
        "train_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"train[:{coco_dataset_ratio}%]\")\n",
        "valid_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"validation[:{coco_dataset_ratio}%]\")\n",
        "test_ds = load_dataset(\"HuggingFaceM4/COCO\", split=\"test\")\n",
        "\n",
        "# Get the number of examples in each split\n",
        "train_len = len(train_ds)\n",
        "valid_len = len(valid_ds)\n",
        "test_len = len(test_ds)\n",
        "\n",
        "train_len, valid_len, test_len  # Display the number of examples in each split"
      ],
      "metadata": {
        "id": "3MNaUVTP2TY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Images with Less than 3 Dimensions"
      ],
      "metadata": {
        "id": "6tyK3oiv3cDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out images with less than 3 dimensions (possibly grayscale images)\n",
        "train_ds = train_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)\n",
        "valid_ds = valid_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)\n",
        "test_ds = test_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)"
      ],
      "metadata": {
        "id": "aykI2EVF3RqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "I-uecfQV35T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(items):\n",
        "    # Preprocess the image\n",
        "    pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "    # Tokenize the captions with truncation and padding\n",
        "    targets = tokenizer([sentence[\"raw\"] for sentence in items[\"sentences\"]],\n",
        "                        max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
        "\n",
        "# Apply the preprocess function to transform the datasets during training\n",
        "train_dataset = train_ds.map(preprocess)\n",
        "valid_dataset = valid_ds.map(preprocess)\n",
        "test_dataset = test_ds.map(preprocess)"
      ],
      "metadata": {
        "id": "fyHxgbuj3sh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Collation Function"
      ],
      "metadata": {
        "id": "HPfVVCiw4YSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.stack([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "# This function takes a batch of preprocessed examples and stacks the pixel values and labels into tensors. It will be used by the data loader to collate the samples into batches."
      ],
      "metadata": {
        "id": "BYpNzhDc3-Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Computation"
      ],
      "metadata": {
        "id": "wftumlXv4xpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "# Load the Rouge and Bleu metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds = eval_pred.label_ids\n",
        "    labels = eval_pred.predictions\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute the Rouge score\n",
        "    rouge_result = rouge.compute(predictions=pred_str, references=labels_str)\n",
        "    rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}  # Multiply by 100 to get the same scale as the Rouge score\n",
        "\n",
        "    # Compute the Bleu score\n",
        "    bleu_result = bleu.compute(predictions=pred_str, references=labels_str)\n",
        "\n",
        "    # Get the length of the generated captions\n",
        "    generation_length = bleu_result[\"translation_length\"]\n",
        "\n",
        "    return {\n",
        "        **rouge_result,\n",
        "        \"bleu\": round(bleu_result[\"bleu\"] * 100, 4),\n",
        "        \"gen_len\": bleu_result[\"translation_length\"] / len(preds)\n",
        "    }\n",
        "\n",
        "# This function takes the evaluation predictions (including label ids and predicted ids) and computes the Rouge and Bleu scores for the generated captions. It also calculates the average generation length."
      ],
      "metadata": {
        "id": "xsoONCcF4s8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters"
      ],
      "metadata": {
        "id": "kw9CUXC25LYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2  # Number of epochs\n",
        "batch_size = 16  # Batch size\n",
        "\n",
        "# Set the number of training epochs and the batch size. Adjust these values according to your specific requirements."
      ],
      "metadata": {
        "id": "YAF6VbgJ5Geb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Example Shapes"
      ],
      "metadata": {
        "id": "G4iK9S0A5kp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the training dataset and print the shapes of labels and pixel values for an example.\n",
        "for item in train_dataset:\n",
        "    print(item[\"labels\"].shape)\n",
        "    print(item[\"pixel_values\"].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "U-GsMu9h5UkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Arguments"
      ],
      "metadata": {
        "id": "MM_cRjXs6Gm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,             # Use generate to calculate the loss\n",
        "    num_train_epochs=num_epochs,            # Number of training epochs\n",
        "    evaluation_strategy=\"steps\",            # Evaluate after each eval_steps\n",
        "    eval_steps=2000,                        # Evaluate after each 2000 steps\n",
        "    logging_steps=2000,                     # Log after each 2000 steps\n",
        "    save_steps=2000,                        # Save after each 2000 steps\n",
        "    per_device_train_batch_size=batch_size, # Batch size for training\n",
        "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
        "    output_dir=\"vit-swin-base-224-gpt2-image-captioning\",  # Output directory for saving checkpoints and logs\n",
        "    # push_to_hub=True # Whether you want to push the model to the hub\n",
        "    # Check this guide for more details: https://huggingface.co/transformers/model_sharing.html\n",
        ")"
      ],
      "metadata": {
        "id": "EzcUfuB454Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader Functions"
      ],
      "metadata": {
        "id": "RSGzJ_Xy6aj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_eval_loader(eval_dataset=None):\n",
        "    return DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "\n",
        "def get_test_loader(eval_dataset=None):\n",
        "    return DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "\n",
        "# Override the `get_train_dataloader`, `get_eval_dataloader`, and `get_test_dataloader` methods of the trainer\n",
        "# so that we can properly load the data\n",
        "\n",
        "trainer.get_train_dataloader = lambda: DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "trainer.get_eval_dataloader = get_eval_loader\n",
        "trainer.get_test_dataloader = get_test_loader\n",
        "\n",
        "# These functions define the data loaders for training, evaluation, and testing. We override the default methods in the trainer to use our custom data loaders that properly collate the batches using the `collate_fn` function."
      ],
      "metadata": {
        "id": "EV90_wBb6WcA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}