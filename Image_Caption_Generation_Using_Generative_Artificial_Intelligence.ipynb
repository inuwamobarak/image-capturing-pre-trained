{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4e+uw5dCO9I58utexIjl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inuwamobarak/image-capturing-pre-trained/blob/main/Image_Caption_Generation_Using_Generative_Artificial_Intelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing requirements"
      ],
      "metadata": {
        "id": "3hfByemHz2b7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9csEh9NyE6h"
      },
      "outputs": [],
      "source": [
        "# Install required packages for the code to run\n",
        "!pip install transformers rouge_score evaluate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and Packages"
      ],
      "metadata": {
        "id": "7ibWfEs6zwkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries and packages\n",
        "\n",
        "import requests  # Library for making HTTP requests\n",
        "import torch  # PyTorch for deep learning\n",
        "from PIL import Image  # Library for image processing\n",
        "from transformers import *  # Transformers library for NLP tasks\n",
        "from tqdm import tqdm  # Library for displaying progress bars\n",
        "import numpy as np # Library for numerical manipulation\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Checking for GPU availability and setting the device accordingly"
      ],
      "metadata": {
        "id": "fW44YhdmybhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Encoders and Decoders"
      ],
      "metadata": {
        "id": "uk6mZald0ZrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model used for encoding the image and extracting image features\n",
        "# Available encoder models:\n",
        "# encoder_model = \"WinKawaks/vit-small-patch16-224\"\n",
        "# encoder_model = \"google/vit-base-patch16-224\"\n",
        "# encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
        "encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
        "\n",
        "# The model used for decoding the image features and generating captions\n",
        "# Available decoder models:\n",
        "# decoder_model = \"bert-base-uncased\"\n",
        "# decoder_model = \"prajjwal1/bert-tiny\"\n",
        "decoder_model = \"gpt2\"\n",
        "\n",
        "## Load the pre-trained Encoder and Decoder models\n",
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    encoder_model, decoder_model\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "orIV7YiIykMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Tokenizers and Image Processors"
      ],
      "metadata": {
        "id": "UMUW2pv61sE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the Tokenizer\n",
        "\n",
        "# The tokenizer is used to preprocess the text and convert it into numerical inputs for the model\n",
        "# Available tokenizers:\n",
        "# tokenizer = AutoTokenizer.from_pretrained(decoder_model)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(decoder_model)\n",
        "\n",
        "## Initialize the Image Processor\n",
        "\n",
        "# The image processor is used to preprocess the images and extract visual features\n",
        "# Available image processors:\n",
        "# - ViTImageProcessor (for \"google/vit-base-patch16-224\" and \"microsoft/swin-base-patch4-window7-224-in22k\" encoder models)\n",
        "image_processor = ViTImageProcessor.from_pretrained(encoder_model)"
      ],
      "metadata": {
        "id": "rNhNiKcx0w7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the Model and Tokenizer for the Decoder"
      ],
      "metadata": {
        "id": "klWjnKQu166B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the decoder model is \"gpt2\"\n",
        "if \"gpt2\" in decoder_model:\n",
        "    # Adjust the tokenizer and model configurations for \"gpt2\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad_token_id as eos_token_id\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model.config.decoder_start_token_id = tokenizer.bos_token_id  # Set decoder_start_token_id as bos_token_id\n",
        "else:\n",
        "    # For other decoder models\n",
        "    model.config.decoder_start_token_id = tokenizer.cls_token_id  # Set decoder_start_token_id as cls_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id  # Set pad_token_id as pad_token_id"
      ],
      "metadata": {
        "id": "hPnmCWIy1x_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and Loading the Dataset"
      ],
      "metadata": {
        "id": "PCPLUZx-2Zyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "max_length = 32  # Maximum length of the captions in tokens\n",
        "coco_dataset_ratio = 50  # 50% of the COCO2014 dataset\n",
        "\n",
        "# Load the COCO2014 dataset for training, validation, and testing splits\n",
        "train_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"train[:{coco_dataset_ratio}%]\")\n",
        "valid_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"validation[:{coco_dataset_ratio}%]\")\n",
        "test_ds = load_dataset(\"HuggingFaceM4/COCO\", split=\"test\")\n",
        "\n",
        "# Get the number of examples in each split\n",
        "train_len = len(train_ds)\n",
        "valid_len = len(valid_ds)\n",
        "test_len = len(test_ds)\n",
        "\n",
        "train_len, valid_len, test_len  # Display the number of examples in each split"
      ],
      "metadata": {
        "id": "3MNaUVTP2TY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Images with Less than 3 Dimensions"
      ],
      "metadata": {
        "id": "6tyK3oiv3cDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out images with less than 3 dimensions (possibly grayscale images)\n",
        "train_ds = train_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)\n",
        "valid_ds = valid_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)\n",
        "test_ds = test_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)"
      ],
      "metadata": {
        "id": "aykI2EVF3RqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "I-uecfQV35T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(items):\n",
        "    # Preprocess the image\n",
        "    pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "    # Tokenize the captions with truncation and padding\n",
        "    targets = tokenizer([sentence[\"raw\"] for sentence in items[\"sentences\"]],\n",
        "                        max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
        "\n",
        "# Apply the preprocess function to transform the datasets during training\n",
        "train_dataset = train_ds.map(preprocess)\n",
        "valid_dataset = valid_ds.map(preprocess)\n",
        "test_dataset = test_ds.map(preprocess)"
      ],
      "metadata": {
        "id": "fyHxgbuj3sh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Collation Function"
      ],
      "metadata": {
        "id": "HPfVVCiw4YSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.stack([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "# This function takes a batch of preprocessed examples and stacks the pixel values and labels into tensors. It will be used by the data loader to collate the samples into batches."
      ],
      "metadata": {
        "id": "BYpNzhDc3-Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Computation"
      ],
      "metadata": {
        "id": "wftumlXv4xpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "# Load the Rouge and Bleu metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds = eval_pred.label_ids\n",
        "    labels = eval_pred.predictions\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute the Rouge score\n",
        "    rouge_result = rouge.compute(predictions=pred_str, references=labels_str)\n",
        "    rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}  # Multiply by 100 to get the same scale as the Rouge score\n",
        "\n",
        "    # Compute the Bleu score\n",
        "    bleu_result = bleu.compute(predictions=pred_str, references=labels_str)\n",
        "\n",
        "    # Get the length of the generated captions\n",
        "    generation_length = bleu_result[\"translation_length\"]\n",
        "\n",
        "    return {\n",
        "        **rouge_result,\n",
        "        \"bleu\": round(bleu_result[\"bleu\"] * 100, 4),\n",
        "        \"gen_len\": bleu_result[\"translation_length\"] / len(preds)\n",
        "    }\n",
        "\n",
        "# This function takes the evaluation predictions (including label ids and predicted ids) and computes the Rouge and Bleu scores for the generated captions. It also calculates the average generation length."
      ],
      "metadata": {
        "id": "xsoONCcF4s8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters"
      ],
      "metadata": {
        "id": "kw9CUXC25LYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2  # Number of epochs\n",
        "batch_size = 16  # Batch size\n",
        "\n",
        "# Set the number of training epochs and the batch size. Adjust these values according to your specific requirements."
      ],
      "metadata": {
        "id": "YAF6VbgJ5Geb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Example Shapes"
      ],
      "metadata": {
        "id": "G4iK9S0A5kp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the training dataset and print the shapes of labels and pixel values for an example.\n",
        "for item in train_dataset:\n",
        "    print(item[\"labels\"].shape)\n",
        "    print(item[\"pixel_values\"].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "U-GsMu9h5UkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Arguments"
      ],
      "metadata": {
        "id": "MM_cRjXs6Gm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,             # Use generate to calculate the loss\n",
        "    num_train_epochs=num_epochs,            # Number of training epochs\n",
        "    evaluation_strategy=\"steps\",            # Evaluate after each eval_steps\n",
        "    eval_steps=2000,                        # Evaluate after each 2000 steps\n",
        "    logging_steps=2000,                     # Log after each 2000 steps\n",
        "    save_steps=2000,                        # Save after each 2000 steps\n",
        "    per_device_train_batch_size=batch_size, # Batch size for training\n",
        "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
        "    output_dir=\"vit-swin-base-224-gpt2-image-captioning\",  # Output directory for saving checkpoints and logs\n",
        "    # push_to_hub=True # Whether you want to push the model to the hub\n",
        "    # Check this guide for more details: https://huggingface.co/transformers/model_sharing.html\n",
        ")"
      ],
      "metadata": {
        "id": "EzcUfuB454Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader Functions"
      ],
      "metadata": {
        "id": "RSGzJ_Xy6aj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_eval_loader(eval_dataset=None):\n",
        "    return DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "\n",
        "def get_test_loader(eval_dataset=None):\n",
        "    return DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "\n",
        "# Override the `get_train_dataloader`, `get_eval_dataloader`, and `get_test_dataloader` methods of the trainer\n",
        "# so that we can properly load the data\n",
        "\n",
        "trainer.get_train_dataloader = lambda: DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "trainer.get_eval_dataloader = get_eval_loader\n",
        "trainer.get_test_dataloader = get_test_loader\n",
        "\n",
        "# These functions define the data loaders for training, evaluation, and testing. We override the default methods in the trainer to use our custom data loaders that properly collate the batches using the `collate_fn` function."
      ],
      "metadata": {
        "id": "EV90_wBb6WcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "0n7Cj3nzdoTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "trainer.evaluate(test_dataset)\n",
        "\n",
        "# If you set the push_to_hub parameter in the TrainingArguments,\n",
        "# complete the push to the model hub using the following code\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "2f1AFtp7czjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Memory by Freeing Space"
      ],
      "metadata": {
        "id": "qqk39LcCdrIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import gc\n",
        "\n",
        "# Free up GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Perform garbage collection to release unused memory\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "SBvLWYiJdCpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PyToch DataLoader"
      ],
      "metadata": {
        "id": "7LuAScaJe_ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data loaders for training, validation, and testing datasets\n",
        "train_dataset_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
        "valid_dataset_loader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=8, shuffle=True)\n",
        "test_dataset_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "18ffHR2Udh3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# The code comment has been updated to explain that the purpose of this line is to define the optimizer (AdamW) and initialize it with the model's parameters, using a learning rate of 1e-5."
      ],
      "metadata": {
        "id": "O09qZwDDeHFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Log Files with TensorBoard"
      ],
      "metadata": {
        "id": "sCNaNxrFfILN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./image-captioning/tensorboard\n",
        "\n",
        "# The code you provided is used to start TensorBoard within a Jupyter Notebook environment. It first loads the TensorBoard extension using `%load_ext tensorboard`, and then starts TensorBoard with the specified log directory using `%tensorboard --logdir ./image-captioning/tensorboard`.\n",
        "\n",
        "# Please note that the `./image-captioning/tensorboard` directory should contain the log files generated during the training process."
      ],
      "metadata": {
        "id": "DpO8JMLDeb9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a SummaryWriter for TensorBoard logging\n",
        "summary_writer = SummaryWriter(log_dir=\"./image-captioning/tensorboard\")\n",
        "\n",
        "# Print some statistics before training\n",
        "# Calculate the number of training steps\n",
        "n_train_steps = num_epochs * len(train_dataset_loader)\n",
        "# Calculate the number of validation steps\n",
        "n_valid_steps = len(valid_dataset_loader)\n",
        "# Set the current training step to 0\n",
        "current_step = 0\n",
        "# Define the step frequency for logging, evaluation, and saving\n",
        "save_steps = 1000\n",
        "\n",
        "\n",
        "# The code comments have been updated to explain that the `SummaryWriter` is used for logging training information to TensorBoard. The statistics before training are also clarified, including the calculation of the number of training and validation steps, the initialization of the current training step, and the definition of the step frequency for logging, evaluation, and saving."
      ],
      "metadata": {
        "id": "23KSHuIifZQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "glRaqclDgkWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the training loss\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataset_loader, \"Training\", total=len(train_dataset_loader), leave=False):\n",
        "        if current_step % save_steps == 0:\n",
        "            # Evaluate on the validation set if the current step is a multiple of the save steps\n",
        "            print()\n",
        "            print(f\"Validation at step {current_step}...\")\n",
        "            print()\n",
        "\n",
        "            # Set the model to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Initialize lists to store predictions and labels for validation\n",
        "            predictions, labels = [], []\n",
        "\n",
        "            # Initialize the validation loss\n",
        "            valid_loss = 0\n",
        "\n",
        "            for batch in valid_dataset_loader:\n",
        "                # Get the batch data\n",
        "                pixel_values = batch[\"pixel_values\"]\n",
        "                label_ids = batch[\"labels\"]\n",
        "\n",
        "                # Perform forward pass\n",
        "                outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = outputs.loss\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                # Free the GPU memory\n",
        "                logits = outputs.logits.detach().cpu()\n",
        "\n",
        "                # Add the predictions to the list\n",
        "                predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "\n",
        "                # Add the labels to the list\n",
        "                labels.extend(label_ids.tolist())\n",
        "\n",
        "            # Create EvalPrediction object for compute_metrics function\n",
        "            eval_prediction = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "\n",
        "            # Compute the metrics\n",
        "            metrics = compute_metrics(eval_prediction)\n",
        "\n",
        "            # Print the statistics\n",
        "            print()\n",
        "            print(f\"Epoch: {epoch}, Step: {current_step}, Train Loss: {train_loss / save_steps:.4f}, \" +\n",
        "                  f\"Valid Loss: {valid_loss / n_valid_steps:.4f}, BLEU: {metrics['bleu']:.4f}, \" +\n",
        "                  f\"ROUGE-1: {metrics['rouge1']:.4f}, ROUGE-2: {metrics['rouge2']:.4f}, ROUGE-L: {metrics['rougeL']:.4f}\")\n",
        "            print()\n",
        "\n",
        "            # Log the metrics to TensorBoard\n",
        "            summary_writer.add_scalar(\"valid_loss\", valid_loss / n_valid_steps, global_step=current_step)\n",
        "            summary_writer.add_scalar(\"bleu\", metrics[\"bleu\"], global_step=current_step)\n",
        "            summary_writer.add_scalar(\"rouge1\", metrics[\"rouge1\"], global_step=current_step)\n",
        "            summary_writer.add_scalar(\"rouge2\", metrics[\"rouge2\"], global_step=current_step)\n",
        "            summary_writer.add_scalar(\"rougeL\", metrics[\"rougeL\"], global_step=current_step)\n",
        "\n",
        "            # Save the model\n",
        "            model.save_pretrained(f\"./image-captioning/checkpoint-{current_step}\")\n",
        "            tokenizer.save_pretrained(f\"./image-captioning/checkpoint-{current_step}\")\n",
        "            image_processor.save_pretrained(f\"./image-captioning/checkpoint-{current_step}\")\n",
        "\n",
        "            # Set the model back to train mode\n",
        "            model.train()\n",
        "\n",
        "            # Reset the train and valid loss\n",
        "            train_loss, valid_loss = 0, 0\n",
        "\n",
        "        # Get the batch data and convert them to tensors\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Perform forward pass\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Log the loss\n",
        "        loss_value = loss.item()\n",
        "        train_loss += loss_value\n",
        "\n",
        "        # Increment the step\n",
        "        current_step += 1\n",
        "\n",
        "        # Log the training loss to TensorBoard\n",
        "        summary_writer.add_scalar(\"train_loss\", loss_value, global_step=current_step)\n",
        "\n",
        "#The training loop code has been updated with clearer comments to describe each step of the loop. It includes training the model, evaluating on the validation set at specified steps, logging metrics to TensorBoard, saving the model, and updating the training loss."
      ],
      "metadata": {
        "id": "MjnUgjRfgWtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Best Model"
      ],
      "metadata": {
        "id": "yaJuThWng1Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model by changing the checkpoint number to the best checkpoint\n",
        "best_checkpoint = 3000\n",
        "best_model = VisionEncoderDecoderModel.from_pretrained(f\"./image-captioning/checkpoint-{best_checkpoint}\").to(device)\n",
        "\n",
        "#The code comment has been updated to explain that the purpose of this code is to load the best model from a specified checkpoint. The checkpoint number is set to the `best_checkpoint` variable, and the model is loaded using the `from_pretrained` method from the `VisionEncoderDecoderModel` class. The loaded model is then assigned to the `best_model` variable and moved to the specified device (GPU in this case)."
      ],
      "metadata": {
        "id": "_6qeBQOkgxly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "7U05izIiiQ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The provided code defines a function `get_evaluation_metrics` that takes a `model` and a `dataset` as inputs and returns evaluation metrics. Here's a revised version of the code with clearer comments:\n",
        "\n",
        "def get_evaluation_metrics(model, dataset):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Define the dataloader\n",
        "    dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
        "\n",
        "    # Calculate the number of testing steps\n",
        "    n_test_steps = len(dataloader)\n",
        "\n",
        "    # Initialize lists to store predictions and labels\n",
        "    predictions, labels = [], []\n",
        "\n",
        "    # Initialize the test loss\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Iterate over batches in the dataloader\n",
        "    for batch in tqdm(dataloader, \"Evaluating\"):\n",
        "        # Get the batch data\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        label_ids = batch[\"labels\"]\n",
        "\n",
        "        # Perform forward pass\n",
        "        outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = outputs.loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Free the GPU memory\n",
        "        logits = outputs.logits.detach().cpu()\n",
        "\n",
        "        # Add the predictions to the list\n",
        "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "\n",
        "        # Add the labels to the list\n",
        "        labels.extend(label_ids.tolist())\n",
        "\n",
        "    # Create EvalPrediction object that the compute_metrics function expects\n",
        "    eval_prediction = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "\n",
        "    # Compute the evaluation metrics\n",
        "    metrics = compute_metrics(eval_prediction)\n",
        "\n",
        "    # Add the test_loss to the metrics\n",
        "    metrics[\"test_loss\"] = test_loss / n_test_steps\n",
        "\n",
        "    return metrics\n",
        "\n",
        "#The revised comments provide clearer explanations of each step in the function."
      ],
      "metadata": {
        "id": "ih2Z8Pw8haE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The code `metrics = get_evaluation_metrics(best_model, test_dataset)` calls the `get_evaluation_metrics` function with the `best_model` and `test_dataset` as inputs, and assigns the returned metrics to the `metrics` variable. Finally, `metrics` is printed to display the evaluation metrics.\n",
        "\n",
        "metrics = get_evaluation_metrics(best_model, test_dataset)\n",
        "metrics"
      ],
      "metadata": {
        "id": "F4T4ngfZhs-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The code `finetuned_metrics = get_evaluation_metrics(finetuned_model, test_dataset)` calls the `get_evaluation_metrics` function with the `finetuned_model` and `test_dataset` as inputs, and assigns the returned metrics to the `finetuned_metrics` variable. Finally, `finetuned_metrics` is printed to display the evaluation metrics.\n",
        "\n",
        "finetuned_metrics = get_evaluation_metrics(finetuned_model, test_dataset)\n",
        "finetuned_metrics"
      ],
      "metadata": {
        "id": "t0U_CQ2xh1ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Pipeline"
      ],
      "metadata": {
        "id": "lk_VQOBuiYv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The provided code sets up an image captioning pipeline using the Hugging Face Transformers library. The pipeline is initialized with the `\"image-to-text\"` task and the `\"Abdou/vit-swin-base-224-gpt2-image-captioning\"` model. Here's the updated code:\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create the image captioning pipeline\n",
        "image_captioner = pipeline(\"image-to-text\", model=\"Abdou/vit-swin-base-224-gpt2-image-captioning\")\n",
        "\n",
        "# Move the model to the specified device (e.g., GPU)\n",
        "image_captioner.model = image_captioner.model.to(device)\n",
        "\n",
        "#The code comment explains that the purpose is to set up an image captioning pipeline using the specified model and move the model to the specified device for inference."
      ],
      "metadata": {
        "id": "Y68u_d-gh5NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_evaluation_metrics(image_captioner.model, test_dataset)"
      ],
      "metadata": {
        "id": "D2uBdyaNiJLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The provided code defines a function `show_image_and_captions` that takes a URL as input. It displays the image, generates captions using different models, and prints the captions. Here's the revised code with clearer comments:\n",
        "\n",
        "def show_image_and_captions(url):\n",
        "    # Get the image and display it\n",
        "    display(load_image(url))\n",
        "\n",
        "    # Generate captions using different models\n",
        "    our_caption = get_caption(best_model, image_processor, tokenizer, url)\n",
        "    finetuned_caption = get_caption(finetuned_model, finetuned_image_processor, finetuned_tokenizer, url)\n",
        "    pipeline_caption = get_caption(image_captioner.model, image_processor, tokenizer, url)\n",
        "\n",
        "    # Print the captions\n",
        "    print(f\"Our caption: {our_caption}\")\n",
        "    print(f\"nlpconnect/vit-gpt2-image-captioning caption: {finetuned_caption}\")\n",
        "    print(f\"Abdou/vit-swin-base-224-gpt2-image-captioning caption: {pipeline_caption}\")\n",
        "\n",
        "#The updated comments provide clearer explanations of each step in the function."
      ],
      "metadata": {
        "id": "FJBHxwRUjGzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The `show_image_and_captions()` function displays the image from the provided URL and generates captions using different models. Here's the updated code:\n",
        "\n",
        "show_image_and_captions(\"http://images.cocodataset.org/test-stuff2017/000000000001.jpg\")\n",
        "\n",
        "#Running this code will display the image and print the captions generated by the `best_model`, `finetuned_model`, and `image_captioner.model` for the given image URL."
      ],
      "metadata": {
        "id": "D0CwgigBjWuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_and_captions(\"http://images.cocodataset.org/test-stuff2017/000000000019.jpg\")"
      ],
      "metadata": {
        "id": "EfH4_ah7j4VH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}